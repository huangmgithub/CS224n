{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick refresher on the word2vec algorithm. The key insight behind word2vec is that *'a word is known by the company it keeps'*. Concretely, suppose we have a 'center' word c and a contextual windown surrouding c. We shall refer to words that lie in this contextual window as 'outside words'. For example, in Figure 1 we see that the center word c is 'banking'. Since the context window size is 2, the outside words are 'turning', 'into', 'crises', and 'as'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the skip-gram word2vec algorithm is to accurately learn the probability distribution P(O|C). Given a specific word o and a specific word c, we want to calculate P(O=o|C=c), which is th probability that word o is an 'outside' word for c, i.e., the probability that o falls within the contextual window of c.  \n",
    "\n",
    "In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P(O=o|C=c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w \\in Vocab} \\exp(u_w^Tv_c)}  \\tag{1}\n",
    "\\label{eq:sample}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $u_o$ is the 'outside' vector representing outside word o, and $v_c$ is the 'center' vector representing center word c. To contain these parameters, we have two matrices, $U$ and $V$. The columns of $U$ are all the 'outside' vectors $u_w$. The columns of $V$ are all of the 'center' vectors $v_w$. Both $U$ and $V$ contain a vector for every $w \\in \\text{Vocabulary}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lectures that, for a single pair of words c and o, the loss is given by:\n",
    "$$\n",
    "J_{naive-softmax}(v_c, o, U) = -\\log P(O=o | C=c) \\tag{2}\n",
    "$$\n",
    "\n",
    "Another way to view this loss is as the cross-entropy between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k^{th}$ entry in these vectors indicates the condition probabiility of the $k^{th}$ word being an 'outside word' for the given c. The true empirical distribution $y$ is a one-hot vector with a 1 for the true outside word o, and 0 everywhere else. The predicted distribution $\\hat{y}$ is the probability $P(O|C=c)$ given by our model in equation(1) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Show that the naive-softmax given in Equation (2) is the same as cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that\n",
    "$$\n",
    "- \\sum_{w \\in \\text{Vocab}} y_w \\log(\\hat{y_w}) = -\\log(\\hat{y_o})\n",
    "$$\n",
    "Your answer should be one line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:    \n",
    "$$ - \\sum_{w \\in \\text{Vocab}} y_w \\log(\\hat{y_w}) = -(y_1 \\log(\\hat{y_1}) + y_2 \\log(\\hat{y_2}) + ... + y_k \\log(\\hat{y_k}) + ... + y_n \\log(\\hat{y_n})) = -y_k \\log(\\hat{y_k})=   -\\log(\\hat{y_o}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**(b)** Compute the partial derivative of $J_{native-softmax}(v_c, o, U)$ with respect to $v_c$. Please write your answer in terms of $y, \\hat{y}$, and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:   \n",
    "[*Note*:Derivatives represent in shape convention of lecture3, shape: y ==> 1xV, U ==> dxV   ]\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_{native-softmax}(v_c, o, U)\n",
    "& = - \\log P(O=o|C=c) \\\\\n",
    "& = - \\log \\frac{\\exp(u_o^Tv_c)}{\\sum_{w \\in Vocab} \\exp(u_w^Tv_c)} \\\\ \n",
    "& = - u_o^Tv_c + \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c) \\\\\n",
    "\\end{aligned} \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u_o^T v_c}{\\partial v_c} = u^T \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\log \\sum_{w \\in Vocab} \\exp(u_w^T v_c)} {\\partial v_c}\n",
    "& = \\frac{1}{\\sum_{w \\in Vocab} \\exp(u_w^T v_c)} \\frac{\\partial \\sum_{w \\in Vocab} \\exp(u_w^T v_c)}{\\partial v_c}\\\\\n",
    "& = \\frac{1}{\\sum_{w \\in Vocab} \\exp(u_w^T v_c)} \\frac{\\partial \\sum_{w \\in Vocab} \\exp(u_w^T v_c)}{\\partial v_c}\\\\\n",
    "& = \\frac{1}{\\sum_{w \\in Vocab} \\exp(u_w^T v_c)} \\sum_{w \\in Vocab} \\frac{\\exp(u_w^T v_c)} {\\partial v_c} \\\\\n",
    "& = \\frac{1}{\\sum_{w \\in Vocab} \\exp(u_w^T v_c)} \\sum_{w \\in Vocab} \\exp(u_w^T v_c)\\frac{\\partial u_w^T v_c} {\\partial v_c} \\\\\n",
    "& = \\frac{1}{\\sum_{w \\in Vocab} \\exp(u_w^T v_c)} \\sum_{w \\in Vocab} \\exp(u_w^T v_c)u_w^T\\\\\n",
    "& = \\sum_{w \\in Vocab} \\frac{\\exp(u_w^T v_c)}{\\sum_{x \\in Vocab} \\exp(u_x^T v_c)} u_w^T\n",
    "\\end{aligned} \\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{native-softmax}(v_c, o, U)}{\\partial v_c} \n",
    "& = \\frac{\\partial - u_o^Tv_c + \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)}{\\partial v_c} \\\\\n",
    "& = -u_o^T + \\sum_{w \\in Vocab} \\frac{\\exp(u_w^T v_c)}{\\sum_{x \\in Vocab} \\exp(u_x^T v_c)} u_w^T \\\\\n",
    "& = -u_o^T + \\sum_{w \\in Vocab} \\hat{y_w} u_w^T \\\\\n",
    "& = (\\hat{y} -y)U^T\n",
    "\\end{aligned}\\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**(c)** Compute the partial derivatives of $J_{naive-softmax}(v_c, o, U)$ with respect to each of the 'outside' word vector, $u_w$'s. There will be two cases: when w=o, the true 'outside' word vector, and $w\\neq o$, for all other words. Please write you answer in terms of $y, \\hat{y}$, and $v_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "$$\n",
    "J_{native-softmax}(v_c, o, U) = - u_o^Tv_c + \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c) \\tag{1}\n",
    "$$\n",
    "+ case (1) $w \\neq o$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{native-softmax}(v_c, o, U)}{\\partial u_w} \n",
    "& = \\frac{\\partial - u_o^Tv_c + \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)} {\\partial u_w}  \\\\\n",
    "& = \\frac{\\partial \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)}{\\partial u_w} \\\\\n",
    "& = \\frac{1}{\\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)} \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)\\frac{\\partial u_w^T v_c}{\\partial u_w} \\\\\n",
    "& = \\frac{1}{\\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)} \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)v_c^T \\\\\n",
    "& = \\sum_{w \\in  Vocab} \\frac{\\exp(u_w^Tv_c)}{\\sum_{x \\in  Vocab} \\exp(u_x^Tv_c)} v_c^T \\\\\n",
    "& = \\hat{y}v_c^T\n",
    "\\end{aligned}\\tag{2}\n",
    "$$\n",
    "\n",
    "+ case (2) $w = o$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{native-softmax}(v_c, o, U)}{\\partial u_w} \n",
    "& = \\frac{\\partial - u_o^Tv_c + \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)} {\\partial u_w}  \\\\\n",
    "& = - \\frac{\\partial u_o^Tv_c}{u_w} + \\frac{\\partial \\log \\sum_{w \\in  Vocab} \\exp(u_w^Tv_c)}{\\partial u_w} \\\\\n",
    "& = -v_c^T +  \\sum_{w \\in  Vocab} \\frac{\\exp(u_w^Tv_c)}{\\sum_{x \\in  Vocab} \\exp(u_x^Tv_c)} v_c^T \\\\\n",
    "& = -v_c^T + \\hat{y} v_c^T \\\\\n",
    "\\end{aligned}\\tag{3}\n",
    "$$\n",
    "\n",
    "+ merge (2) and (3):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{native-softmax}(v_c, o, U)}{\\partial u_w}= (\\hat{y} - y)v_c^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**(d)** The sigmoid function is given by Equation 4:\n",
    "    $$\n",
    "    \\delta(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x + 1}\n",
    "    $$\n",
    "    Please compute the derivative of $\\delta(x)$ with respect to $x$, where $x$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\delta(x)}{\\partial x}\n",
    "& = \\frac{\\partial \\frac{e^x}{e^x + 1}}{\\partial x} \\\\\n",
    "& = \\frac{e^x(e^x +1) - e^xe^x}{(e^x + 1)^2} \\\\\n",
    "& = \\frac{e^x}{(e^x + 1)(e^x +1)} \\\\\n",
    "& = \\delta(x)(1 - \\delta(x))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**(e)** Now we shall consider th Negtive Sampling loss, which is an alternative to the Naive Softmax loss. Assume that K negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1, w_2, ..., w_k$ and their outside vectors as $u_1, ..., u_k$. Note that $o \\notin {w_1,...w_k}$. For a center word c and an outside word o, the negative sampling loss function is given by:\n",
    "$$\n",
    "J_{neg-sample(v_c, o, U)} = -\\log(\\delta(u_o^T v_c))-\\sum_{k=1}^K \\log(\\delta(-u_k^Tv_c))\n",
    "$$\n",
    "for a sample $w_1, ... w_K$, where $\\delta(\\cdot)$ is the sigmoid function.\n",
    "\n",
    "Please repeat parts(b) and (c), computing the partial derivatives of $J_{neg-sample}$ with respect $v_c$, with respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the vectors $u_o, v_c$ and $u_k$, where $k \\in [1, K]$. After you've done this, describe with one sentence why this loss function is much more efficient to compute that the naive-softmax loss. Note, you should be able to use your solution to part(d) to help compute the necessary gradients here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "+ (1) Partial derivatives of $J_{neg-sample}$ with respect $v_c$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{neg-sample(v_c, o, U)}}{\\partial v_c} \n",
    "& = \\frac{\\partial -\\log(\\delta(u_o^T v_c))- \\sum_{k=1}^K \\log(\\delta(-u_k^T v_c))}{\\partial v_c} \\\\\n",
    "& = - \\frac{\\partial \\log(\\delta(u_o^T v_c))}{ \\partial v_c}  - \\frac{\\partial \\sum_{k=1}^K \\log(\\delta(-u_k^T v_c))}{\\partial v_c}\\\\\n",
    "& = - \\frac{1}{\\delta(u_o^T v_c)}\\delta(u_o^T v_c)(1-\\delta(u_o^T v_c))\\frac{\\partial u_o^T v_c}{\\partial v_c} - \\sum_{k=1}^K \\frac{1}{\\delta(-u_k^T v_c)}\\delta(-u_k^T v_c)(1 - \\delta(-u_k^T v_c))\\frac{\\partial (-u_k^T v_c)}{\\partial v_c} \\\\\n",
    "& = (\\delta(u_o^T v_c) - 1)u_o^T  - \\sum_{k=1}^K(1 - \\delta(-u_k^T v_c))(-u_k^T) \\\\\n",
    "& = (\\delta(u_o^T v_c) - 1)u_o^T  + \\sum_{k=1}^K \\delta(u_k^T v_c)u_k^T \n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ (2) Partial derivatives of $J_{neg-sample}$ with respect $u_o$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{neg-sample(v_c, o, U)}}{\\partial u_o} \n",
    "& = \\frac{\\partial -\\log(\\delta(u_o^T v_c))- \\sum_{k=1}^K \\log(\\delta(-u_k^T v_c))}{\\partial u_o} \\\\ \n",
    "& = \\frac{\\partial -\\log(\\delta(u_o^T v_c))}{\\partial u_o}  - \\frac{\\partial \\sum_{k=1}^K \\log(\\delta(-u_k^T v_c))}{\\partial u_o}  \\\\\n",
    "& = (\\delta(u_o^Tv_c) - 1) v_c^T\n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ (3) Partial derivatives of $J_{neg-sample}$ with respect $u_k$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J_{neg-sample(v_c, o, U)}}{\\partial u_k} \n",
    "& = \\frac{\\partial -\\log(\\delta(u_o^T v_c))- \\sum_{k=1}^K \\log(\\delta(-u_k^T v_c))}{\\partial u_k} \\\\ \n",
    "& = \\frac{\\partial -\\log(\\delta(u_o^T v_c))}{\\partial u_k}  - \\frac{\\partial \\sum_{k=1}^K \\log(\\delta(-u_k^T v_c))}{\\partial u_k}  \\\\\n",
    "& = \\sum_{k=1}^K \\delta(u_k^Tv_c)v_c^T\n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ (4) why more efficient? Because negative sampling training neural network only updates the true outside vector and negtive samples's, much less than Vocabulary * dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**(f)** Suppose the center word is $c=w_t$ and the context window is $[w_{t-m}, ..., w_{t-1}, w_t, w_{t+1}, ..., w_{t+m}]$, where m is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:\n",
    "$$\n",
    "J_{skip-gram}(v_c, w_{t-m}, ... w_{t+m}, U) = \\sum_{-m \\leq j \\leq m, j\\neq 0} J(v_c, w_{t+j}, U) \\tag{6}\n",
    "$$\n",
    "\n",
    "Here $J(v_c, w_{t+j}, U)$ represents an arbitary loss term for the center word $c=w_t$ and outside word $w_{t+j}$.  \n",
    "\n",
    "$J(v_c, w_{t+j}, U)$ could be $J_{naive-softmax}(v_c, w_{t+j}, U)$ or $J_{neg-sample}(v_c, w_{t+j}, U)$, depending on your implementation.\n",
    "\n",
    "Write down three partial derivatives:   \n",
    "(i)  $\\partial J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) / \\partial U$   \n",
    "(ii) $\\partial J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) / \\partial v_c$   \n",
    "(iii) $\\partial J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) / \\partial v_w $ when $w \\neq c$.\n",
    "\n",
    "Write your answers in terms of  $\\partial J(v_c, w_{t+j}, U) / \\partial U$ and $\\partial J(v_c, w_{t+j}, U) / \\partial v_c$. This is very simle each solution should be one line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "+ (i) \n",
    "$\n",
    "\\partial J_{skip-gram}(v_c, w_{t-m}, ...,w_{t+m}, U)/\\partial U = \\sum_{-m \\leq j \\leq m, j\\neq 0} \\partial J(v_c, w_{t+j}, U)/\\partial U  \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ (ii) $ \\partial J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) / \\partial v_c = \\sum_{-m \\leq j \\leq m, j\\neq 0} \\partial J(v_c, w_{t+j}, U)/\\partial v_c  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ (ii) $ \\partial J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) / \\partial v_w = O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
